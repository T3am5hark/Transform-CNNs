{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.virtualenvs/gpu2/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/.virtualenvs/gpu2/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "#from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from kerasutils import hits_and_misses, describe_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 1)\n",
      "[6]\n",
      "[8]\n",
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "print(trainY.shape)\n",
    "print(testY.shape)\n",
    "print(trainY[0])\n",
    "print(testY[1])\n",
    "\n",
    "input_shape = trainX[0,:,:,:].shape\n",
    "print(input_shape)\n",
    "#print(trainX[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Transform labels from int to one-hot vectors\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.fit_transform(testY)\n",
    "\n",
    "print(trainY.shape)\n",
    "print(testY.shape)\n",
    "n_classes = trainY.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN architecture with Keras\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=trainX[0,:,:,:].shape, filters=32, \n",
    "                 use_bias=True, kernel_size=(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=96, use_bias=False, kernel_size=(5,5), strides=2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters=96, use_bias=False, kernel_size=(5,5), strides=2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Conv2D(filters=64, use_bias=False, kernel_size=(3,3), strides=3))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.05))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Conv2D(filters=64, use_bias=False, kernel_size=(3,3), strides=2))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper architecture\n",
    "# https://arxiv.org/pdf/1412.6806.pdf\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=trainX[0,:,:,:].shape, filters=96, kernel_size=(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(filters=96, kernel_size=(3,3), strides=2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters=192, kernel_size=(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(filters=192, kernel_size=(3,3), strides=2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "basefilename = 'cifar10-cnn'\n",
    "filepath=basefilename+\".{acc:.4f}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/500\n",
      "196/196 [==============================] - 23s 117ms/step - loss: 1.7835 - acc: 0.3463 - val_loss: 1.3726 - val_acc: 0.4972\n",
      "Epoch 2/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 1.4484 - acc: 0.4771 - val_loss: 1.2765 - val_acc: 0.5399\n",
      "Epoch 3/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 1.3114 - acc: 0.5322 - val_loss: 1.1251 - val_acc: 0.5996\n",
      "Epoch 4/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 1.2311 - acc: 0.5629 - val_loss: 0.9650 - val_acc: 0.6562\n",
      "Epoch 5/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 1.1595 - acc: 0.5897 - val_loss: 1.0204 - val_acc: 0.6310\n",
      "Epoch 6/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 1.1226 - acc: 0.6027 - val_loss: 0.9367 - val_acc: 0.6681\n",
      "Epoch 7/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 1.0844 - acc: 0.6152 - val_loss: 0.9365 - val_acc: 0.6731\n",
      "Epoch 8/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 1.0575 - acc: 0.6287 - val_loss: 0.9581 - val_acc: 0.6715\n",
      "Epoch 9/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 1.0355 - acc: 0.6342 - val_loss: 0.8170 - val_acc: 0.7118\n",
      "Epoch 10/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 1.0151 - acc: 0.6422 - val_loss: 0.8560 - val_acc: 0.6994\n",
      "Epoch 11/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 1.0001 - acc: 0.6476 - val_loss: 0.8754 - val_acc: 0.6987\n",
      "Epoch 12/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.9821 - acc: 0.6572 - val_loss: 0.8820 - val_acc: 0.6869\n",
      "Epoch 13/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.9605 - acc: 0.6628 - val_loss: 0.7932 - val_acc: 0.7198\n",
      "Epoch 14/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.9519 - acc: 0.6668 - val_loss: 0.8431 - val_acc: 0.7114\n",
      "Epoch 15/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.9378 - acc: 0.6681 - val_loss: 0.7318 - val_acc: 0.7432\n",
      "Epoch 16/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.9322 - acc: 0.6744 - val_loss: 0.7999 - val_acc: 0.7220\n",
      "Epoch 17/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.9189 - acc: 0.6782 - val_loss: 0.7646 - val_acc: 0.7338\n",
      "Epoch 18/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.9110 - acc: 0.6826 - val_loss: 0.7236 - val_acc: 0.7508\n",
      "Epoch 19/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.9023 - acc: 0.6853 - val_loss: 0.8091 - val_acc: 0.7211\n",
      "Epoch 20/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.8895 - acc: 0.6889 - val_loss: 0.6790 - val_acc: 0.7631\n",
      "Epoch 21/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8871 - acc: 0.6890 - val_loss: 0.7224 - val_acc: 0.7489\n",
      "Epoch 22/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.8759 - acc: 0.6943 - val_loss: 0.7149 - val_acc: 0.7536\n",
      "Epoch 23/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8593 - acc: 0.6997 - val_loss: 0.7401 - val_acc: 0.7464\n",
      "Epoch 24/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8662 - acc: 0.6996 - val_loss: 0.6752 - val_acc: 0.7645\n",
      "Epoch 25/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8602 - acc: 0.6992 - val_loss: 0.7213 - val_acc: 0.7546\n",
      "Epoch 26/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.8470 - acc: 0.7031 - val_loss: 0.7576 - val_acc: 0.7444\n",
      "Epoch 27/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8437 - acc: 0.7041 - val_loss: 0.7399 - val_acc: 0.7488\n",
      "Epoch 28/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.8389 - acc: 0.7066 - val_loss: 0.6776 - val_acc: 0.7651\n",
      "Epoch 29/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.8338 - acc: 0.7088 - val_loss: 0.6484 - val_acc: 0.7772\n",
      "Epoch 30/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8206 - acc: 0.7111 - val_loss: 0.7831 - val_acc: 0.7290\n",
      "Epoch 31/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8361 - acc: 0.7073 - val_loss: 0.7348 - val_acc: 0.7509\n",
      "Epoch 32/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8194 - acc: 0.7146 - val_loss: 0.6625 - val_acc: 0.7752\n",
      "Epoch 33/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.8134 - acc: 0.7169 - val_loss: 0.7187 - val_acc: 0.7538\n",
      "Epoch 34/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8141 - acc: 0.7146 - val_loss: 0.6121 - val_acc: 0.7882\n",
      "Epoch 35/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.8066 - acc: 0.7187 - val_loss: 0.6580 - val_acc: 0.7800\n",
      "Epoch 36/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8019 - acc: 0.7193 - val_loss: 0.6268 - val_acc: 0.7874\n",
      "Epoch 37/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7978 - acc: 0.7210 - val_loss: 0.6486 - val_acc: 0.7806\n",
      "Epoch 38/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8033 - acc: 0.7225 - val_loss: 0.6885 - val_acc: 0.7646\n",
      "Epoch 39/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7940 - acc: 0.7230 - val_loss: 0.7091 - val_acc: 0.7566\n",
      "Epoch 40/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7855 - acc: 0.7263 - val_loss: 0.6490 - val_acc: 0.7809\n",
      "Epoch 41/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7850 - acc: 0.7273 - val_loss: 0.5888 - val_acc: 0.7960\n",
      "Epoch 42/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7828 - acc: 0.7283 - val_loss: 0.6472 - val_acc: 0.7790\n",
      "Epoch 43/500\n",
      "196/196 [==============================] - 22s 114ms/step - loss: 0.7788 - acc: 0.7281 - val_loss: 0.6311 - val_acc: 0.7824\n",
      "Epoch 44/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7789 - acc: 0.7300 - val_loss: 0.6268 - val_acc: 0.7854\n",
      "Epoch 45/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7685 - acc: 0.7314 - val_loss: 0.6091 - val_acc: 0.7906\n",
      "Epoch 46/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7754 - acc: 0.7311 - val_loss: 0.5941 - val_acc: 0.7970\n",
      "Epoch 47/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7708 - acc: 0.7295 - val_loss: 0.6376 - val_acc: 0.7839\n",
      "Epoch 48/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7674 - acc: 0.7353 - val_loss: 0.6783 - val_acc: 0.7691\n",
      "Epoch 49/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7598 - acc: 0.7360 - val_loss: 0.6718 - val_acc: 0.7722\n",
      "Epoch 50/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7592 - acc: 0.7374 - val_loss: 0.6666 - val_acc: 0.7776\n",
      "Epoch 51/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7706 - acc: 0.7298 - val_loss: 0.6016 - val_acc: 0.7962\n",
      "Epoch 52/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7535 - acc: 0.7375 - val_loss: 0.6610 - val_acc: 0.7777\n",
      "Epoch 53/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7627 - acc: 0.7359 - val_loss: 0.5888 - val_acc: 0.8056\n",
      "Epoch 54/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7591 - acc: 0.7342 - val_loss: 0.6083 - val_acc: 0.7968\n",
      "Epoch 55/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7481 - acc: 0.7402 - val_loss: 0.7086 - val_acc: 0.7545\n",
      "Epoch 56/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7465 - acc: 0.7396 - val_loss: 0.5626 - val_acc: 0.8058\n",
      "Epoch 57/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7400 - acc: 0.7432 - val_loss: 0.5838 - val_acc: 0.8028\n",
      "Epoch 58/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7497 - acc: 0.7412 - val_loss: 0.5478 - val_acc: 0.8116\n",
      "Epoch 59/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7431 - acc: 0.7410 - val_loss: 0.5859 - val_acc: 0.8017\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7365 - acc: 0.7438 - val_loss: 0.5918 - val_acc: 0.7967\n",
      "Epoch 61/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7378 - acc: 0.7422 - val_loss: 0.6083 - val_acc: 0.7965\n",
      "Epoch 62/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7358 - acc: 0.7459 - val_loss: 0.6348 - val_acc: 0.7848\n",
      "Epoch 63/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7387 - acc: 0.7431 - val_loss: 0.5813 - val_acc: 0.8054\n",
      "Epoch 64/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7301 - acc: 0.7479 - val_loss: 0.6413 - val_acc: 0.7799\n",
      "Epoch 65/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7336 - acc: 0.7454 - val_loss: 0.5931 - val_acc: 0.7988\n",
      "Epoch 66/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7258 - acc: 0.7468 - val_loss: 0.6193 - val_acc: 0.7928\n",
      "Epoch 67/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7258 - acc: 0.7491 - val_loss: 0.6000 - val_acc: 0.8019\n",
      "Epoch 68/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7269 - acc: 0.7487 - val_loss: 0.5499 - val_acc: 0.8132\n",
      "Epoch 69/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7279 - acc: 0.7462 - val_loss: 0.6325 - val_acc: 0.7928\n",
      "Epoch 70/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7191 - acc: 0.7515 - val_loss: 0.5626 - val_acc: 0.8130\n",
      "Epoch 71/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7200 - acc: 0.7512 - val_loss: 0.5916 - val_acc: 0.7994\n",
      "Epoch 72/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7226 - acc: 0.7484 - val_loss: 0.6056 - val_acc: 0.7943\n",
      "Epoch 73/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7160 - acc: 0.7543 - val_loss: 0.6268 - val_acc: 0.7893\n",
      "Epoch 74/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7131 - acc: 0.7514 - val_loss: 0.5777 - val_acc: 0.8070\n",
      "Epoch 75/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7162 - acc: 0.7530 - val_loss: 0.5832 - val_acc: 0.8016\n",
      "Epoch 76/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7086 - acc: 0.7543 - val_loss: 0.5935 - val_acc: 0.7987\n",
      "Epoch 77/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7156 - acc: 0.7518 - val_loss: 0.5787 - val_acc: 0.8003\n",
      "Epoch 78/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7141 - acc: 0.7516 - val_loss: 0.6604 - val_acc: 0.7807\n",
      "Epoch 79/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7050 - acc: 0.7540 - val_loss: 0.5855 - val_acc: 0.8067\n",
      "Epoch 80/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7069 - acc: 0.7530 - val_loss: 0.5710 - val_acc: 0.8064\n",
      "Epoch 81/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7096 - acc: 0.7539 - val_loss: 0.5982 - val_acc: 0.7993\n",
      "Epoch 82/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7084 - acc: 0.7539 - val_loss: 0.5702 - val_acc: 0.8077\n",
      "Epoch 83/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7012 - acc: 0.7574 - val_loss: 0.5467 - val_acc: 0.8149\n",
      "Epoch 84/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7014 - acc: 0.7575 - val_loss: 0.5841 - val_acc: 0.8056\n",
      "Epoch 85/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6982 - acc: 0.7583 - val_loss: 0.6163 - val_acc: 0.7925\n",
      "Epoch 86/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7043 - acc: 0.7547 - val_loss: 0.5837 - val_acc: 0.8013\n",
      "Epoch 87/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7037 - acc: 0.7549 - val_loss: 0.5710 - val_acc: 0.8074\n",
      "Epoch 88/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6973 - acc: 0.7586 - val_loss: 0.5719 - val_acc: 0.8077\n",
      "Epoch 89/500\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.6984 - acc: 0.7574 - val_loss: 0.5793 - val_acc: 0.8068\n",
      "Epoch 90/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6955 - acc: 0.7598 - val_loss: 0.6118 - val_acc: 0.7943\n",
      "Epoch 91/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6963 - acc: 0.7604 - val_loss: 0.5810 - val_acc: 0.8056\n",
      "Epoch 92/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6945 - acc: 0.7590 - val_loss: 0.5421 - val_acc: 0.8192\n",
      "Epoch 93/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6878 - acc: 0.7615 - val_loss: 0.5553 - val_acc: 0.8106\n",
      "Epoch 94/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6906 - acc: 0.7614 - val_loss: 0.6008 - val_acc: 0.7998\n",
      "Epoch 95/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6899 - acc: 0.7606 - val_loss: 0.5487 - val_acc: 0.8143\n",
      "Epoch 96/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6893 - acc: 0.7634 - val_loss: 0.5698 - val_acc: 0.8088\n",
      "Epoch 97/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6884 - acc: 0.7607 - val_loss: 0.5469 - val_acc: 0.8150\n",
      "Epoch 98/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6852 - acc: 0.7610 - val_loss: 0.5867 - val_acc: 0.7996\n",
      "Epoch 99/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6884 - acc: 0.7620 - val_loss: 0.5390 - val_acc: 0.8203\n",
      "Epoch 100/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6877 - acc: 0.7632 - val_loss: 0.5963 - val_acc: 0.7979\n",
      "Epoch 101/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6830 - acc: 0.7636 - val_loss: 0.5651 - val_acc: 0.8127\n",
      "Epoch 102/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6812 - acc: 0.7613 - val_loss: 0.5426 - val_acc: 0.8189\n",
      "Epoch 103/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6762 - acc: 0.7646 - val_loss: 0.5300 - val_acc: 0.8209\n",
      "Epoch 104/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6744 - acc: 0.7645 - val_loss: 0.5701 - val_acc: 0.8119\n",
      "Epoch 105/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6766 - acc: 0.7658 - val_loss: 0.5871 - val_acc: 0.8050\n",
      "Epoch 106/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6769 - acc: 0.7656 - val_loss: 0.5273 - val_acc: 0.8241\n",
      "Epoch 107/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6756 - acc: 0.7648 - val_loss: 0.5217 - val_acc: 0.8250\n",
      "Epoch 108/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6794 - acc: 0.7655 - val_loss: 0.5420 - val_acc: 0.8189\n",
      "Epoch 109/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6729 - acc: 0.7661 - val_loss: 0.5299 - val_acc: 0.8219\n",
      "Epoch 110/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6783 - acc: 0.7660 - val_loss: 0.5334 - val_acc: 0.8179\n",
      "Epoch 111/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6659 - acc: 0.7691 - val_loss: 0.5204 - val_acc: 0.8234\n",
      "Epoch 112/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6757 - acc: 0.7656 - val_loss: 0.5343 - val_acc: 0.8231\n",
      "Epoch 113/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6711 - acc: 0.7677 - val_loss: 0.5910 - val_acc: 0.8024\n",
      "Epoch 114/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6738 - acc: 0.7657 - val_loss: 0.5361 - val_acc: 0.8205\n",
      "Epoch 115/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6695 - acc: 0.7689 - val_loss: 0.5690 - val_acc: 0.8111\n",
      "Epoch 116/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6658 - acc: 0.7680 - val_loss: 0.5253 - val_acc: 0.8222\n",
      "Epoch 117/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6615 - acc: 0.7709 - val_loss: 0.6226 - val_acc: 0.7979\n",
      "Epoch 118/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6641 - acc: 0.7684 - val_loss: 0.4826 - val_acc: 0.8394\n",
      "Epoch 119/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6715 - acc: 0.7646 - val_loss: 0.5409 - val_acc: 0.8225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6674 - acc: 0.7680 - val_loss: 0.5414 - val_acc: 0.8205\n",
      "Epoch 121/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6674 - acc: 0.7675 - val_loss: 0.5283 - val_acc: 0.8207\n",
      "Epoch 122/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6633 - acc: 0.7718 - val_loss: 0.5135 - val_acc: 0.8285\n",
      "Epoch 123/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6658 - acc: 0.7699 - val_loss: 0.5273 - val_acc: 0.8212\n",
      "Epoch 124/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6612 - acc: 0.7727 - val_loss: 0.5502 - val_acc: 0.8183\n",
      "Epoch 125/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6622 - acc: 0.7717 - val_loss: 0.5195 - val_acc: 0.8275\n",
      "Epoch 126/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6559 - acc: 0.7725 - val_loss: 0.5469 - val_acc: 0.8170\n",
      "Epoch 127/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6592 - acc: 0.7742 - val_loss: 0.5210 - val_acc: 0.8258\n",
      "Epoch 128/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6608 - acc: 0.7703 - val_loss: 0.5359 - val_acc: 0.8189\n",
      "Epoch 129/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6651 - acc: 0.7719 - val_loss: 0.5270 - val_acc: 0.8271\n",
      "Epoch 130/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6618 - acc: 0.7696 - val_loss: 0.5436 - val_acc: 0.8181\n",
      "Epoch 131/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6597 - acc: 0.7718 - val_loss: 0.5269 - val_acc: 0.8223\n",
      "Epoch 132/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6557 - acc: 0.7720 - val_loss: 0.5412 - val_acc: 0.8200\n",
      "Epoch 133/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6558 - acc: 0.7727 - val_loss: 0.5019 - val_acc: 0.8327\n",
      "Epoch 134/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6573 - acc: 0.7728 - val_loss: 0.5319 - val_acc: 0.8203\n",
      "Epoch 135/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6563 - acc: 0.7734 - val_loss: 0.5545 - val_acc: 0.8160\n",
      "Epoch 136/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6518 - acc: 0.7696 - val_loss: 0.5504 - val_acc: 0.8179\n",
      "Epoch 137/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6532 - acc: 0.7745 - val_loss: 0.5298 - val_acc: 0.8171\n",
      "Epoch 138/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6566 - acc: 0.7718 - val_loss: 0.5218 - val_acc: 0.8253\n",
      "Epoch 139/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6566 - acc: 0.7736 - val_loss: 0.5188 - val_acc: 0.8245\n",
      "Epoch 140/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6501 - acc: 0.7754 - val_loss: 0.5332 - val_acc: 0.8252\n",
      "Epoch 141/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6548 - acc: 0.7723 - val_loss: 0.4930 - val_acc: 0.8352\n",
      "Epoch 142/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6469 - acc: 0.7757 - val_loss: 0.5944 - val_acc: 0.8085\n",
      "Epoch 143/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6513 - acc: 0.7758 - val_loss: 0.5072 - val_acc: 0.8277\n",
      "Epoch 144/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6555 - acc: 0.7754 - val_loss: 0.5384 - val_acc: 0.8189\n",
      "Epoch 145/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6541 - acc: 0.7738 - val_loss: 0.5300 - val_acc: 0.8293\n",
      "Epoch 146/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6469 - acc: 0.7748 - val_loss: 0.4963 - val_acc: 0.8310\n",
      "Epoch 147/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6473 - acc: 0.7763 - val_loss: 0.5148 - val_acc: 0.8261\n",
      "Epoch 148/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6471 - acc: 0.7757 - val_loss: 0.5045 - val_acc: 0.8284\n",
      "Epoch 149/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6442 - acc: 0.7759 - val_loss: 0.5558 - val_acc: 0.8166\n",
      "Epoch 150/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6451 - acc: 0.7749 - val_loss: 0.5611 - val_acc: 0.8116\n",
      "Epoch 151/500\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.6399 - acc: 0.7789 - val_loss: 0.4944 - val_acc: 0.8357\n",
      "Epoch 152/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6417 - acc: 0.7762 - val_loss: 0.5725 - val_acc: 0.8085\n",
      "Epoch 153/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6379 - acc: 0.7765 - val_loss: 0.5036 - val_acc: 0.8316\n",
      "Epoch 154/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6393 - acc: 0.7779 - val_loss: 0.5328 - val_acc: 0.8224\n",
      "Epoch 155/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6446 - acc: 0.7771 - val_loss: 0.5028 - val_acc: 0.8290\n",
      "Epoch 156/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6402 - acc: 0.7775 - val_loss: 0.5147 - val_acc: 0.8289\n",
      "Epoch 157/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6386 - acc: 0.7782 - val_loss: 0.5546 - val_acc: 0.8167\n",
      "Epoch 158/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6409 - acc: 0.7770 - val_loss: 0.5319 - val_acc: 0.8236\n",
      "Epoch 159/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6406 - acc: 0.7781 - val_loss: 0.5330 - val_acc: 0.8251\n",
      "Epoch 160/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6436 - acc: 0.7763 - val_loss: 0.5389 - val_acc: 0.8215\n",
      "Epoch 161/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6410 - acc: 0.7791 - val_loss: 0.5041 - val_acc: 0.8334\n",
      "Epoch 162/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6351 - acc: 0.7796 - val_loss: 0.5117 - val_acc: 0.8277\n",
      "Epoch 163/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6394 - acc: 0.7783 - val_loss: 0.5237 - val_acc: 0.8269\n",
      "Epoch 164/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6356 - acc: 0.7797 - val_loss: 0.5172 - val_acc: 0.8293\n",
      "Epoch 165/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6338 - acc: 0.7805 - val_loss: 0.4895 - val_acc: 0.8342\n",
      "Epoch 166/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6327 - acc: 0.7813 - val_loss: 0.5371 - val_acc: 0.8192\n",
      "Epoch 167/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6330 - acc: 0.7801 - val_loss: 0.5129 - val_acc: 0.8281\n",
      "Epoch 168/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6332 - acc: 0.7817 - val_loss: 0.5191 - val_acc: 0.8265\n",
      "Epoch 169/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6242 - acc: 0.7854 - val_loss: 0.5309 - val_acc: 0.8232\n",
      "Epoch 170/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6339 - acc: 0.7806 - val_loss: 0.4884 - val_acc: 0.8367\n",
      "Epoch 171/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6273 - acc: 0.7844 - val_loss: 0.5159 - val_acc: 0.8257\n",
      "Epoch 172/500\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.6319 - acc: 0.7790 - val_loss: 0.4672 - val_acc: 0.8422\n",
      "Epoch 173/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6228 - acc: 0.7855 - val_loss: 0.4873 - val_acc: 0.8343\n",
      "Epoch 174/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6251 - acc: 0.7807 - val_loss: 0.4898 - val_acc: 0.8381\n",
      "Epoch 175/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6245 - acc: 0.7836 - val_loss: 0.5028 - val_acc: 0.8318\n",
      "Epoch 176/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6265 - acc: 0.7818 - val_loss: 0.5429 - val_acc: 0.8206\n",
      "Epoch 177/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6301 - acc: 0.7820 - val_loss: 0.4965 - val_acc: 0.8313\n",
      "Epoch 178/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6286 - acc: 0.7839 - val_loss: 0.5254 - val_acc: 0.8251\n",
      "Epoch 179/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6247 - acc: 0.7837 - val_loss: 0.5581 - val_acc: 0.8143\n",
      "Epoch 180/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6234 - acc: 0.7835 - val_loss: 0.5401 - val_acc: 0.8224\n",
      "Epoch 181/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6278 - acc: 0.7833 - val_loss: 0.5099 - val_acc: 0.8298\n",
      "Epoch 182/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6299 - acc: 0.7820 - val_loss: 0.5138 - val_acc: 0.8301\n",
      "Epoch 183/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6255 - acc: 0.7820 - val_loss: 0.5301 - val_acc: 0.8212\n",
      "Epoch 184/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6308 - acc: 0.7822 - val_loss: 0.5051 - val_acc: 0.8306\n",
      "Epoch 185/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6197 - acc: 0.7854 - val_loss: 0.4771 - val_acc: 0.8374\n",
      "Epoch 186/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6267 - acc: 0.7837 - val_loss: 0.5191 - val_acc: 0.8276\n",
      "Epoch 187/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6254 - acc: 0.7837 - val_loss: 0.5751 - val_acc: 0.8105\n",
      "Epoch 188/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6206 - acc: 0.7848 - val_loss: 0.4987 - val_acc: 0.8315\n",
      "Epoch 189/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6194 - acc: 0.7843 - val_loss: 0.5294 - val_acc: 0.8213\n",
      "Epoch 190/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6240 - acc: 0.7843 - val_loss: 0.4933 - val_acc: 0.8299\n",
      "Epoch 191/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6228 - acc: 0.7844 - val_loss: 0.4798 - val_acc: 0.8388\n",
      "Epoch 192/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6161 - acc: 0.7865 - val_loss: 0.5011 - val_acc: 0.8288\n",
      "Epoch 193/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6217 - acc: 0.7825 - val_loss: 0.4889 - val_acc: 0.8341\n",
      "Epoch 194/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6218 - acc: 0.7833 - val_loss: 0.5416 - val_acc: 0.8225\n",
      "Epoch 195/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6170 - acc: 0.7872 - val_loss: 0.4966 - val_acc: 0.8328\n",
      "Epoch 196/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6162 - acc: 0.7857 - val_loss: 0.5496 - val_acc: 0.8212\n",
      "Epoch 197/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6212 - acc: 0.7845 - val_loss: 0.5355 - val_acc: 0.8210\n",
      "Epoch 198/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6195 - acc: 0.7832 - val_loss: 0.4943 - val_acc: 0.8359\n",
      "Epoch 199/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6124 - acc: 0.7868 - val_loss: 0.5213 - val_acc: 0.8257\n",
      "Epoch 200/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6212 - acc: 0.7865 - val_loss: 0.5327 - val_acc: 0.8201\n",
      "Epoch 201/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6098 - acc: 0.7877 - val_loss: 0.4814 - val_acc: 0.8400\n",
      "Epoch 202/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6121 - acc: 0.7890 - val_loss: 0.5602 - val_acc: 0.8112\n",
      "Epoch 203/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6163 - acc: 0.7855 - val_loss: 0.5467 - val_acc: 0.8143\n",
      "Epoch 204/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6165 - acc: 0.7868 - val_loss: 0.4864 - val_acc: 0.8372\n",
      "Epoch 205/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6177 - acc: 0.7854 - val_loss: 0.4791 - val_acc: 0.8388\n",
      "Epoch 206/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6212 - acc: 0.7857 - val_loss: 0.5105 - val_acc: 0.8250\n",
      "Epoch 207/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6042 - acc: 0.7911 - val_loss: 0.5929 - val_acc: 0.8043\n",
      "Epoch 208/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6187 - acc: 0.7836 - val_loss: 0.4623 - val_acc: 0.8453\n",
      "Epoch 209/500\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.6110 - acc: 0.7884 - val_loss: 0.4774 - val_acc: 0.8367\n",
      "Epoch 210/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6094 - acc: 0.7883 - val_loss: 0.4785 - val_acc: 0.8369\n",
      "Epoch 211/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6092 - acc: 0.7872 - val_loss: 0.4987 - val_acc: 0.8334\n",
      "Epoch 212/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6125 - acc: 0.7876 - val_loss: 0.5026 - val_acc: 0.8310\n",
      "Epoch 213/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6129 - acc: 0.7881 - val_loss: 0.5014 - val_acc: 0.8289\n",
      "Epoch 214/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6129 - acc: 0.7872 - val_loss: 0.4791 - val_acc: 0.8392\n",
      "Epoch 215/500\n",
      "196/196 [==============================] - 22s 114ms/step - loss: 0.6098 - acc: 0.7883 - val_loss: 0.5051 - val_acc: 0.8285\n",
      "Epoch 216/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6070 - acc: 0.7907 - val_loss: 0.4927 - val_acc: 0.8338\n",
      "Epoch 217/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6109 - acc: 0.7901 - val_loss: 0.4964 - val_acc: 0.8346\n",
      "Epoch 218/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6065 - acc: 0.7908 - val_loss: 0.4854 - val_acc: 0.8392\n",
      "Epoch 219/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6055 - acc: 0.7895 - val_loss: 0.5139 - val_acc: 0.8293\n",
      "Epoch 220/500\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.6093 - acc: 0.7888 - val_loss: 0.5125 - val_acc: 0.8264\n",
      "Epoch 221/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6074 - acc: 0.7884 - val_loss: 0.4754 - val_acc: 0.8390\n",
      "Epoch 222/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6134 - acc: 0.7856 - val_loss: 0.4723 - val_acc: 0.8394\n",
      "Epoch 223/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6042 - acc: 0.7886 - val_loss: 0.4881 - val_acc: 0.8358\n",
      "Epoch 224/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6041 - acc: 0.7909 - val_loss: 0.4927 - val_acc: 0.8396\n",
      "Epoch 225/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6136 - acc: 0.7893 - val_loss: 0.4960 - val_acc: 0.8328\n",
      "Epoch 226/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6078 - acc: 0.7887 - val_loss: 0.4991 - val_acc: 0.8333\n",
      "Epoch 227/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6044 - acc: 0.7898 - val_loss: 0.4793 - val_acc: 0.8368\n",
      "Epoch 228/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6042 - acc: 0.7902 - val_loss: 0.4740 - val_acc: 0.8379\n",
      "Epoch 229/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6043 - acc: 0.7887 - val_loss: 0.4665 - val_acc: 0.8427\n",
      "Epoch 230/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6023 - acc: 0.7908 - val_loss: 0.4703 - val_acc: 0.8404\n",
      "Epoch 231/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6049 - acc: 0.7898 - val_loss: 0.4943 - val_acc: 0.8373\n",
      "Epoch 232/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6027 - acc: 0.7926 - val_loss: 0.4779 - val_acc: 0.8389\n",
      "Epoch 233/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6016 - acc: 0.7913 - val_loss: 0.4792 - val_acc: 0.8376\n",
      "Epoch 234/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5943 - acc: 0.7934 - val_loss: 0.4630 - val_acc: 0.8419\n",
      "Epoch 235/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5990 - acc: 0.7932 - val_loss: 0.4910 - val_acc: 0.8353\n",
      "Epoch 236/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6069 - acc: 0.7886 - val_loss: 0.4877 - val_acc: 0.8365\n",
      "Epoch 237/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6030 - acc: 0.7893 - val_loss: 0.4880 - val_acc: 0.8382\n",
      "Epoch 238/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5977 - acc: 0.7918 - val_loss: 0.5015 - val_acc: 0.8336\n",
      "Epoch 239/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6046 - acc: 0.7910 - val_loss: 0.4739 - val_acc: 0.8385\n",
      "Epoch 240/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5962 - acc: 0.7918 - val_loss: 0.4952 - val_acc: 0.8384\n",
      "Epoch 241/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5986 - acc: 0.7931 - val_loss: 0.4655 - val_acc: 0.8410\n",
      "Epoch 242/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6010 - acc: 0.7909 - val_loss: 0.5165 - val_acc: 0.8264\n",
      "Epoch 243/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.6057 - acc: 0.7917 - val_loss: 0.4800 - val_acc: 0.8399\n",
      "Epoch 244/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5979 - acc: 0.7933 - val_loss: 0.5043 - val_acc: 0.8322\n",
      "Epoch 245/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.6003 - acc: 0.7934 - val_loss: 0.4881 - val_acc: 0.8339\n",
      "Epoch 246/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6002 - acc: 0.7929 - val_loss: 0.5227 - val_acc: 0.8246\n",
      "Epoch 247/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5984 - acc: 0.7942 - val_loss: 0.5519 - val_acc: 0.8199\n",
      "Epoch 248/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5956 - acc: 0.7931 - val_loss: 0.5299 - val_acc: 0.8238\n",
      "Epoch 249/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5942 - acc: 0.7931 - val_loss: 0.4688 - val_acc: 0.8416\n",
      "Epoch 250/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5988 - acc: 0.7936 - val_loss: 0.4867 - val_acc: 0.8400\n",
      "Epoch 251/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5979 - acc: 0.7926 - val_loss: 0.5085 - val_acc: 0.8348\n",
      "Epoch 252/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5942 - acc: 0.7927 - val_loss: 0.5254 - val_acc: 0.8267\n",
      "Epoch 253/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5981 - acc: 0.7928 - val_loss: 0.4590 - val_acc: 0.8469\n",
      "Epoch 254/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5954 - acc: 0.7943 - val_loss: 0.4815 - val_acc: 0.8397\n",
      "Epoch 255/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5987 - acc: 0.7928 - val_loss: 0.4836 - val_acc: 0.8397\n",
      "Epoch 256/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5948 - acc: 0.7926 - val_loss: 0.4844 - val_acc: 0.8400\n",
      "Epoch 257/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5929 - acc: 0.7956 - val_loss: 0.5114 - val_acc: 0.8320\n",
      "Epoch 258/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.6009 - acc: 0.7922 - val_loss: 0.4804 - val_acc: 0.8414\n",
      "Epoch 259/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5936 - acc: 0.7950 - val_loss: 0.4863 - val_acc: 0.8402\n",
      "Epoch 260/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5971 - acc: 0.7942 - val_loss: 0.4927 - val_acc: 0.8361\n",
      "Epoch 261/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5963 - acc: 0.7921 - val_loss: 0.4920 - val_acc: 0.8379\n",
      "Epoch 262/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5886 - acc: 0.7963 - val_loss: 0.5178 - val_acc: 0.8287\n",
      "Epoch 263/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5915 - acc: 0.7939 - val_loss: 0.5018 - val_acc: 0.8329\n",
      "Epoch 264/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5898 - acc: 0.7961 - val_loss: 0.4625 - val_acc: 0.8455\n",
      "Epoch 265/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5891 - acc: 0.7953 - val_loss: 0.4822 - val_acc: 0.8413\n",
      "Epoch 266/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5880 - acc: 0.7957 - val_loss: 0.4867 - val_acc: 0.8384\n",
      "Epoch 267/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5912 - acc: 0.7964 - val_loss: 0.5215 - val_acc: 0.8273\n",
      "Epoch 268/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5857 - acc: 0.7980 - val_loss: 0.4852 - val_acc: 0.8392\n",
      "Epoch 269/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5904 - acc: 0.7956 - val_loss: 0.5186 - val_acc: 0.8252\n",
      "Epoch 270/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5878 - acc: 0.7960 - val_loss: 0.5389 - val_acc: 0.8266\n",
      "Epoch 271/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5932 - acc: 0.7945 - val_loss: 0.4700 - val_acc: 0.8413\n",
      "Epoch 272/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5962 - acc: 0.7920 - val_loss: 0.4792 - val_acc: 0.8381\n",
      "Epoch 273/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5863 - acc: 0.7991 - val_loss: 0.4888 - val_acc: 0.8369\n",
      "Epoch 274/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5905 - acc: 0.7962 - val_loss: 0.4821 - val_acc: 0.8354\n",
      "Epoch 275/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5877 - acc: 0.7972 - val_loss: 0.4779 - val_acc: 0.8403\n",
      "Epoch 276/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5860 - acc: 0.7971 - val_loss: 0.5006 - val_acc: 0.8340\n",
      "Epoch 277/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5955 - acc: 0.7940 - val_loss: 0.4766 - val_acc: 0.8389\n",
      "Epoch 278/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5928 - acc: 0.7943 - val_loss: 0.4866 - val_acc: 0.8355\n",
      "Epoch 279/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5886 - acc: 0.7958 - val_loss: 0.4567 - val_acc: 0.8471\n",
      "Epoch 280/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5794 - acc: 0.8003 - val_loss: 0.5186 - val_acc: 0.8276\n",
      "Epoch 281/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5831 - acc: 0.7984 - val_loss: 0.4818 - val_acc: 0.8405\n",
      "Epoch 282/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5841 - acc: 0.7965 - val_loss: 0.4506 - val_acc: 0.8476\n",
      "Epoch 283/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5827 - acc: 0.7977 - val_loss: 0.4742 - val_acc: 0.8422\n",
      "Epoch 284/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5932 - acc: 0.7957 - val_loss: 0.4377 - val_acc: 0.8515\n",
      "Epoch 285/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5876 - acc: 0.7956 - val_loss: 0.4551 - val_acc: 0.8461\n",
      "Epoch 286/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5809 - acc: 0.7990 - val_loss: 0.4905 - val_acc: 0.8359\n",
      "Epoch 287/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5886 - acc: 0.7956 - val_loss: 0.4800 - val_acc: 0.8409\n",
      "Epoch 288/500\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.5874 - acc: 0.7971 - val_loss: 0.4660 - val_acc: 0.8429\n",
      "Epoch 289/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5764 - acc: 0.7995 - val_loss: 0.4690 - val_acc: 0.8430\n",
      "Epoch 290/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5867 - acc: 0.7967 - val_loss: 0.4984 - val_acc: 0.8358\n",
      "Epoch 291/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5763 - acc: 0.8005 - val_loss: 0.4710 - val_acc: 0.8416\n",
      "Epoch 292/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5789 - acc: 0.8000 - val_loss: 0.4429 - val_acc: 0.8528\n",
      "Epoch 293/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5845 - acc: 0.7988 - val_loss: 0.4637 - val_acc: 0.8449\n",
      "Epoch 294/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5812 - acc: 0.7984 - val_loss: 0.4996 - val_acc: 0.8366\n",
      "Epoch 295/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5815 - acc: 0.7992 - val_loss: 0.4362 - val_acc: 0.8550\n",
      "Epoch 296/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5823 - acc: 0.7989 - val_loss: 0.4843 - val_acc: 0.8380\n",
      "Epoch 297/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5781 - acc: 0.7976 - val_loss: 0.4652 - val_acc: 0.8456\n",
      "Epoch 298/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5876 - acc: 0.7969 - val_loss: 0.4848 - val_acc: 0.8366\n",
      "Epoch 299/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5814 - acc: 0.7983 - val_loss: 0.5136 - val_acc: 0.8262\n",
      "Epoch 300/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5808 - acc: 0.7998 - val_loss: 0.4525 - val_acc: 0.8488\n",
      "Epoch 301/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5845 - acc: 0.7981 - val_loss: 0.4840 - val_acc: 0.8395\n",
      "Epoch 302/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5797 - acc: 0.7989 - val_loss: 0.4589 - val_acc: 0.8424\n",
      "Epoch 303/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5863 - acc: 0.7974 - val_loss: 0.4796 - val_acc: 0.8394\n",
      "Epoch 304/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5810 - acc: 0.7975 - val_loss: 0.5305 - val_acc: 0.8256\n",
      "Epoch 305/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5709 - acc: 0.8028 - val_loss: 0.4725 - val_acc: 0.8468\n",
      "Epoch 306/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5803 - acc: 0.7983 - val_loss: 0.4856 - val_acc: 0.8374\n",
      "Epoch 307/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5791 - acc: 0.7989 - val_loss: 0.4821 - val_acc: 0.8375\n",
      "Epoch 308/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5721 - acc: 0.8004 - val_loss: 0.4497 - val_acc: 0.8500\n",
      "Epoch 309/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5777 - acc: 0.7985 - val_loss: 0.4946 - val_acc: 0.8332\n",
      "Epoch 310/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5777 - acc: 0.8002 - val_loss: 0.4688 - val_acc: 0.8431\n",
      "Epoch 311/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5743 - acc: 0.8017 - val_loss: 0.5085 - val_acc: 0.8310\n",
      "Epoch 312/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5850 - acc: 0.7958 - val_loss: 0.4566 - val_acc: 0.8492\n",
      "Epoch 313/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5738 - acc: 0.8004 - val_loss: 0.4425 - val_acc: 0.8490\n",
      "Epoch 314/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5774 - acc: 0.8000 - val_loss: 0.4442 - val_acc: 0.8520\n",
      "Epoch 315/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5753 - acc: 0.8016 - val_loss: 0.4481 - val_acc: 0.8513\n",
      "Epoch 316/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5756 - acc: 0.7990 - val_loss: 0.4896 - val_acc: 0.8376\n",
      "Epoch 317/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5769 - acc: 0.7996 - val_loss: 0.5124 - val_acc: 0.8288\n",
      "Epoch 323/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5749 - acc: 0.8027 - val_loss: 0.4805 - val_acc: 0.8344\n",
      "Epoch 324/500\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.5671 - acc: 0.8020 - val_loss: 0.4640 - val_acc: 0.8457\n",
      "Epoch 325/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5709 - acc: 0.8034 - val_loss: 0.4922 - val_acc: 0.8362\n",
      "Epoch 326/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5657 - acc: 0.8022 - val_loss: 0.4637 - val_acc: 0.8464\n",
      "Epoch 327/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5694 - acc: 0.8034 - val_loss: 0.4683 - val_acc: 0.8453\n",
      "Epoch 328/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5636 - acc: 0.8054 - val_loss: 0.4975 - val_acc: 0.8356\n",
      "Epoch 329/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5783 - acc: 0.7989 - val_loss: 0.4672 - val_acc: 0.8453\n",
      "Epoch 330/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5742 - acc: 0.8018 - val_loss: 0.4609 - val_acc: 0.8459\n",
      "Epoch 331/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5728 - acc: 0.8017 - val_loss: 0.4562 - val_acc: 0.8487\n",
      "Epoch 332/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5732 - acc: 0.8012 - val_loss: 0.4392 - val_acc: 0.8513\n",
      "Epoch 333/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5734 - acc: 0.8003 - val_loss: 0.4918 - val_acc: 0.8362\n",
      "Epoch 334/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5681 - acc: 0.8027 - val_loss: 0.4582 - val_acc: 0.8475\n",
      "Epoch 335/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5680 - acc: 0.8032 - val_loss: 0.5529 - val_acc: 0.8198\n",
      "Epoch 336/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5700 - acc: 0.8017 - val_loss: 0.4520 - val_acc: 0.8491\n",
      "Epoch 337/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5691 - acc: 0.8048 - val_loss: 0.4446 - val_acc: 0.8494\n",
      "Epoch 338/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5747 - acc: 0.7998 - val_loss: 0.4645 - val_acc: 0.8470\n",
      "Epoch 339/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5634 - acc: 0.8045 - val_loss: 0.4745 - val_acc: 0.8410\n",
      "Epoch 340/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5671 - acc: 0.8051 - val_loss: 0.4637 - val_acc: 0.8450\n",
      "Epoch 341/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5614 - acc: 0.8045 - val_loss: 0.4836 - val_acc: 0.8359\n",
      "Epoch 342/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5657 - acc: 0.8040 - val_loss: 0.4663 - val_acc: 0.8442\n",
      "Epoch 343/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5669 - acc: 0.8017 - val_loss: 0.4941 - val_acc: 0.8371\n",
      "Epoch 344/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5687 - acc: 0.8022 - val_loss: 0.4550 - val_acc: 0.8466\n",
      "Epoch 345/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5649 - acc: 0.8050 - val_loss: 0.4750 - val_acc: 0.8394\n",
      "Epoch 346/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5710 - acc: 0.8020 - val_loss: 0.4743 - val_acc: 0.8370\n",
      "Epoch 347/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5710 - acc: 0.8013 - val_loss: 0.4692 - val_acc: 0.8452\n",
      "Epoch 348/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5667 - acc: 0.8043 - val_loss: 0.4642 - val_acc: 0.8471\n",
      "Epoch 349/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5632 - acc: 0.8039 - val_loss: 0.4664 - val_acc: 0.8498\n",
      "Epoch 350/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5665 - acc: 0.8049 - val_loss: 0.4526 - val_acc: 0.8496\n",
      "Epoch 351/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5655 - acc: 0.8048 - val_loss: 0.4350 - val_acc: 0.8533\n",
      "Epoch 352/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5553 - acc: 0.8069 - val_loss: 0.4658 - val_acc: 0.8433\n",
      "Epoch 353/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5612 - acc: 0.8045 - val_loss: 0.5168 - val_acc: 0.8312\n",
      "Epoch 354/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5680 - acc: 0.8052 - val_loss: 0.4546 - val_acc: 0.8484\n",
      "Epoch 355/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5663 - acc: 0.8040 - val_loss: 0.4593 - val_acc: 0.8469\n",
      "Epoch 356/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5723 - acc: 0.7991 - val_loss: 0.4667 - val_acc: 0.8448\n",
      "Epoch 357/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5629 - acc: 0.8034 - val_loss: 0.4558 - val_acc: 0.8493\n",
      "Epoch 358/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5710 - acc: 0.8013 - val_loss: 0.4720 - val_acc: 0.8447\n",
      "Epoch 359/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5712 - acc: 0.8011 - val_loss: 0.4562 - val_acc: 0.8499\n",
      "Epoch 360/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5664 - acc: 0.8036 - val_loss: 0.5187 - val_acc: 0.8321\n",
      "Epoch 361/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5647 - acc: 0.8028 - val_loss: 0.4683 - val_acc: 0.8434\n",
      "Epoch 362/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5673 - acc: 0.8030 - val_loss: 0.4544 - val_acc: 0.8471\n",
      "Epoch 363/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5610 - acc: 0.8050 - val_loss: 0.4524 - val_acc: 0.8480\n",
      "Epoch 364/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5690 - acc: 0.8024 - val_loss: 0.4617 - val_acc: 0.8441\n",
      "Epoch 365/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5601 - acc: 0.8043 - val_loss: 0.4824 - val_acc: 0.8386\n",
      "Epoch 366/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5651 - acc: 0.8051 - val_loss: 0.4458 - val_acc: 0.8509\n",
      "Epoch 367/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5618 - acc: 0.8048 - val_loss: 0.4619 - val_acc: 0.8443\n",
      "Epoch 368/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5669 - acc: 0.8022 - val_loss: 0.5001 - val_acc: 0.8353\n",
      "Epoch 369/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5662 - acc: 0.8050 - val_loss: 0.4400 - val_acc: 0.8537\n",
      "Epoch 370/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5659 - acc: 0.8031 - val_loss: 0.4445 - val_acc: 0.8513\n",
      "Epoch 371/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5643 - acc: 0.8052 - val_loss: 0.4514 - val_acc: 0.8505\n",
      "Epoch 372/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5608 - acc: 0.8039 - val_loss: 0.4828 - val_acc: 0.8401\n",
      "Epoch 373/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5577 - acc: 0.8078 - val_loss: 0.4440 - val_acc: 0.8534\n",
      "Epoch 374/500\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.5671 - acc: 0.8055 - val_loss: 0.4402 - val_acc: 0.8516\n",
      "Epoch 375/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5676 - acc: 0.8025 - val_loss: 0.4723 - val_acc: 0.8444\n",
      "Epoch 376/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5531 - acc: 0.8091 - val_loss: 0.4393 - val_acc: 0.8522\n",
      "Epoch 377/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5588 - acc: 0.8065 - val_loss: 0.4651 - val_acc: 0.8424\n",
      "Epoch 378/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5583 - acc: 0.8051 - val_loss: 0.4529 - val_acc: 0.8489\n",
      "Epoch 379/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5544 - acc: 0.8069 - val_loss: 0.4829 - val_acc: 0.8383\n",
      "Epoch 380/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5634 - acc: 0.8049 - val_loss: 0.4419 - val_acc: 0.8543\n",
      "Epoch 381/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5597 - acc: 0.8071 - val_loss: 0.4731 - val_acc: 0.8415\n",
      "Epoch 382/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5643 - acc: 0.8032 - val_loss: 0.4644 - val_acc: 0.8443\n",
      "Epoch 383/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5591 - acc: 0.8047 - val_loss: 0.4356 - val_acc: 0.8550\n",
      "Epoch 389/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5549 - acc: 0.8078 - val_loss: 0.4831 - val_acc: 0.8421\n",
      "Epoch 390/500\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.5566 - acc: 0.8073 - val_loss: 0.4593 - val_acc: 0.8472\n",
      "Epoch 391/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5538 - acc: 0.8080 - val_loss: 0.4405 - val_acc: 0.8551\n",
      "Epoch 392/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5563 - acc: 0.8079 - val_loss: 0.4516 - val_acc: 0.8527\n",
      "Epoch 393/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5580 - acc: 0.8062 - val_loss: 0.4689 - val_acc: 0.8437\n",
      "Epoch 394/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5567 - acc: 0.8058 - val_loss: 0.4545 - val_acc: 0.8508\n",
      "Epoch 395/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5607 - acc: 0.8048 - val_loss: 0.4534 - val_acc: 0.8497\n",
      "Epoch 396/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5549 - acc: 0.8077 - val_loss: 0.4426 - val_acc: 0.8519\n",
      "Epoch 397/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5546 - acc: 0.8087 - val_loss: 0.4756 - val_acc: 0.8449\n",
      "Epoch 398/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5587 - acc: 0.8056 - val_loss: 0.4550 - val_acc: 0.8478\n",
      "Epoch 399/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5543 - acc: 0.8093 - val_loss: 0.4512 - val_acc: 0.8501\n",
      "Epoch 400/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5522 - acc: 0.8074 - val_loss: 0.4897 - val_acc: 0.8387\n",
      "Epoch 401/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5573 - acc: 0.8062 - val_loss: 0.4275 - val_acc: 0.8562\n",
      "Epoch 402/500\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.5570 - acc: 0.8070 - val_loss: 0.4541 - val_acc: 0.8512\n",
      "Epoch 403/500\n",
      "103/196 [==============>...............] - ETA: 9s - loss: 0.5635 - acc: 0.8059 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5538 - acc: 0.8080 - val_loss: 0.4789 - val_acc: 0.8399\n",
      "Epoch 419/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5595 - acc: 0.8065 - val_loss: 0.4709 - val_acc: 0.8450\n",
      "Epoch 420/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5502 - acc: 0.8067 - val_loss: 0.4547 - val_acc: 0.8493\n",
      "Epoch 421/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5577 - acc: 0.8089 - val_loss: 0.4960 - val_acc: 0.8351\n",
      "Epoch 422/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5467 - acc: 0.8094 - val_loss: 0.4559 - val_acc: 0.8531\n",
      "Epoch 423/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5462 - acc: 0.8108 - val_loss: 0.5113 - val_acc: 0.8305\n",
      "Epoch 424/500\n",
      "196/196 [==============================] - 22s 114ms/step - loss: 0.5554 - acc: 0.8080 - val_loss: 0.4565 - val_acc: 0.8495\n",
      "Epoch 425/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5507 - acc: 0.8100 - val_loss: 0.4551 - val_acc: 0.8451\n",
      "Epoch 426/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5421 - acc: 0.8121 - val_loss: 0.4647 - val_acc: 0.8445\n",
      "Epoch 427/500\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.5522 - acc: 0.8095 - val_loss: 0.4799 - val_acc: 0.8423\n",
      "Epoch 428/500\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.5511 - acc: 0.8090 - val_loss: 0.5047 - val_acc: 0.8379\n",
      "Epoch 429/500\n",
      "124/196 [=================>............] - ETA: 7s - loss: 0.5434 - acc: 0.8119"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "batch_size = 256\n",
    "\n",
    "print('Using real-time data augmentation.')\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    zoom_range=0.2,     # zoom image\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(trainX)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(trainX, trainY,\n",
    "                                 batch_size=batch_size),\n",
    "                                 epochs=n_epochs,\n",
    "                                 validation_data=(testX, testY))\n",
    "\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 16s 322us/step - loss: 1.6566 - acc: 0.3988 - val_loss: 1.3428 - val_acc: 0.5170\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 1.3235 - acc: 0.5295 - val_loss: 1.1718 - val_acc: 0.5841\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 1.1789 - acc: 0.5852 - val_loss: 1.0722 - val_acc: 0.6203\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 1.0708 - acc: 0.6237 - val_loss: 0.9994 - val_acc: 0.6508\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 1.0061 - acc: 0.6453 - val_loss: 0.9809 - val_acc: 0.6576\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.9378 - acc: 0.6709 - val_loss: 0.9432 - val_acc: 0.6698\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.8797 - acc: 0.6901 - val_loss: 0.9138 - val_acc: 0.6853\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.8375 - acc: 0.7057 - val_loss: 0.9240 - val_acc: 0.6783\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.7813 - acc: 0.7219 - val_loss: 0.8996 - val_acc: 0.6847\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 8s 160us/step - loss: 0.7521 - acc: 0.7331 - val_loss: 0.9021 - val_acc: 0.6849\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.7135 - acc: 0.7469 - val_loss: 0.8963 - val_acc: 0.6894\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.6739 - acc: 0.7571 - val_loss: 0.8793 - val_acc: 0.7022\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.6454 - acc: 0.7710 - val_loss: 0.8857 - val_acc: 0.6978\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.6099 - acc: 0.7798 - val_loss: 0.8964 - val_acc: 0.7017\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 8s 160us/step - loss: 0.5834 - acc: 0.7913 - val_loss: 0.8934 - val_acc: 0.6985\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.5548 - acc: 0.7997 - val_loss: 0.9089 - val_acc: 0.7029\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.5309 - acc: 0.8076 - val_loss: 0.9208 - val_acc: 0.6982\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.5088 - acc: 0.8159 - val_loss: 0.9357 - val_acc: 0.7027\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.4822 - acc: 0.8228 - val_loss: 0.9845 - val_acc: 0.6925\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.4675 - acc: 0.8284 - val_loss: 0.9574 - val_acc: 0.6987\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.4437 - acc: 0.8380 - val_loss: 0.9588 - val_acc: 0.7078\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.4272 - acc: 0.8449 - val_loss: 0.9858 - val_acc: 0.7049\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.4188 - acc: 0.8457 - val_loss: 0.9854 - val_acc: 0.7036\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.3991 - acc: 0.8524 - val_loss: 1.0283 - val_acc: 0.6967\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.3827 - acc: 0.8612 - val_loss: 1.0385 - val_acc: 0.6973\n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "batch_size = 256\n",
    "callbacks_list = None\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY), \n",
    "              epochs=n_epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
    "print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Evaluating network...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.83      0.80      1000\n",
      "          1       0.85      0.92      0.88      1000\n",
      "          2       0.77      0.58      0.66      1000\n",
      "          3       0.66      0.54      0.59      1000\n",
      "          4       0.72      0.72      0.72      1000\n",
      "          5       0.72      0.63      0.67      1000\n",
      "          6       0.69      0.91      0.78      1000\n",
      "          7       0.81      0.83      0.82      1000\n",
      "          8       0.85      0.87      0.86      1000\n",
      "          9       0.82      0.86      0.84      1000\n",
      "\n",
      "avg / total       0.77      0.77      0.76     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate TEST model class prediction accuracy\n",
    "print(\"[INFO] Evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=batch_size)\n",
    "target_names = [str(x) for x in lb.classes_]\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1),\n",
    "                            target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Evaluating network...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.86      0.85      5000\n",
      "          1       0.87      0.94      0.90      5000\n",
      "          2       0.83      0.66      0.74      5000\n",
      "          3       0.71      0.60      0.65      5000\n",
      "          4       0.77      0.78      0.77      5000\n",
      "          5       0.76      0.66      0.71      5000\n",
      "          6       0.72      0.93      0.81      5000\n",
      "          7       0.83      0.86      0.84      5000\n",
      "          8       0.89      0.91      0.90      5000\n",
      "          9       0.85      0.89      0.87      5000\n",
      "\n",
      "avg / total       0.81      0.81      0.80     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate TRAIN model class prediction accuracy\n",
    "print(\"[INFO] Evaluating network...\")\n",
    "trainPreds = model.predict(trainX, batch_size=batch_size)\n",
    "target_names = [str(x) for x in lb.classes_]\n",
    "print(classification_report(trainY.argmax(axis=1),\n",
    "                            trainPreds.argmax(axis=1),\n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: <class 'keras.layers.convolutional.Conv2D'>\n",
      "    input=(None, 32, 32, 3)\n",
      "    output=(None, 30, 30, 32)\n",
      "    act=<function linear at 0x7f3149bcdae8>\n",
      "    strides=(1, 1)\n",
      "Layer 1: <class 'keras.layers.core.Activation'>\n",
      "    input=(None, 30, 30, 32)\n",
      "    output=(None, 30, 30, 32)\n",
      "    act=<function relu at 0x7f3149bcd8c8>\n",
      "Layer 2: <class 'keras.layers.core.Dropout'>\n",
      "    input=(None, 30, 30, 32)\n",
      "    output=(None, 30, 30, 32)\n",
      "    rate=0.2\n",
      "Layer 3: <class 'keras.layers.convolutional.Conv2D'>\n",
      "    input=(None, 30, 30, 32)\n",
      "    output=(None, 13, 13, 96)\n",
      "    act=<function linear at 0x7f3149bcdae8>\n",
      "    strides=(2, 2)\n",
      "Layer 4: <class 'keras.layers.core.Activation'>\n",
      "    input=(None, 13, 13, 96)\n",
      "    output=(None, 13, 13, 96)\n",
      "    act=<function relu at 0x7f3149bcd8c8>\n",
      "Layer 5: <class 'keras.layers.core.Dropout'>\n",
      "    input=(None, 13, 13, 96)\n",
      "    output=(None, 13, 13, 96)\n",
      "    rate=0.2\n",
      "Layer 6: <class 'keras.layers.core.Flatten'>\n",
      "    input=(None, 13, 13, 96)\n",
      "    output=(None, 16224)\n",
      "Layer 7: <class 'keras.layers.normalization.BatchNormalization'>\n",
      "    input=(None, 16224)\n",
      "    output=(None, 16224)\n",
      "Layer 8: <class 'keras.layers.core.Dense'>\n",
      "    input=(None, 16224)\n",
      "    output=(None, 256)\n",
      "    act=<function linear at 0x7f3149bcdae8>\n",
      "Layer 9: <class 'keras.layers.core.Activation'>\n",
      "    input=(None, 256)\n",
      "    output=(None, 256)\n",
      "    act=<function relu at 0x7f3149bcd8c8>\n",
      "Layer 10: <class 'keras.layers.core.Dropout'>\n",
      "    input=(None, 256)\n",
      "    output=(None, 256)\n",
      "    rate=0.4\n",
      "Layer 11: <class 'keras.layers.core.Dense'>\n",
      "    input=(None, 256)\n",
      "    output=(None, 10)\n",
      "    act=<function softmax at 0x7f3149bcd620>\n"
     ]
    }
   ],
   "source": [
    "from kerasutils import describe_model\n",
    "\n",
    "describe_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "{'seed': None, 'noise_shape': None, 'rate': 0.1, 'trainable': True, 'name': 'dropout_1'}\n"
     ]
    }
   ],
   "source": [
    "layer = model.layers[2]\n",
    "layer.rate = 0.1\n",
    "\n",
    "print(layer.rate)\n",
    "print(layer.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6775 hits, 3225 misses (0.6775%)\n"
     ]
    }
   ],
   "source": [
    "preds = predictions.argmax(axis=1)\n",
    "targets = testY.argmax(axis=1)\n",
    "hits, misses = hits_and_misses(preds, targets)\n",
    "print('{0} hits, {1} misses ({2}%)'.format(len(hits), len(misses), len(hits)/float(len(predictions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0, Predicted: 3\n"
     ]
    }
   ],
   "source": [
    "# Look at examples of misses\n",
    "from imageutils import bgr2rgb\n",
    "idx = misses[random.randint(0,len(misses))]\n",
    "im = testX[idx]\n",
    "plt.figure(figsize=(1.5,1.5))\n",
    "plt.imshow(bgr2rgb(im))\n",
    "plt.show()\n",
    "print('Target: {0}, Predicted: {1}'.format(targets[idx], preds[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 6, Predicted: 6\n"
     ]
    }
   ],
   "source": [
    "# Look at examples of hits\n",
    "from imageutils import bgr2rgb\n",
    "idx = hits[random.randint(0,len(hits))]\n",
    "im = testX[idx]\n",
    "plt.figure(figsize=(1.5,1.5))\n",
    "plt.imshow(bgr2rgb(im))\n",
    "plt.show()\n",
    "print('Target: {0}, Predicted: {1}'.format(targets[idx], preds[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
