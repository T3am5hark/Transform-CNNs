{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from kerasutils import plot_training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/jdm/cnn/Transform-CNNs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282865\n"
     ]
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "#path = '../../data/'\n",
    "#filename = \"ml.txt\"\n",
    "path = './'\n",
    "filename = '@dril.2018-04-21.txt'\n",
    "raw_text = open(path+filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  282865\n",
      "Total Vocab:  77\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', 'ê', '̶', '—', '…', '™', '▀', '▄', '█', '▐', '░', '⸻', '》']\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i,c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i,c in enumerate(chars))\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  282835\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 30\n",
    "dataX = []\n",
    "dataY = []\n",
    "#max_index = 12000\n",
    "# Use this for full document\n",
    "max_index = n_chars-seq_length\n",
    "for i in range(0, max_index, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (282835, 30, 1)\n",
      "y shape = (282835, 77)\n",
      "(30, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "print(\"X shape = {0}\".format(X.shape))\n",
    "print(\"y shape = {0}\".format(y.shape))\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 30, 1)\n",
      "(12500, 30, 1)\n",
      "(12500, 77)\n",
      "(12500, 77)\n"
     ]
    }
   ],
   "source": [
    "#print(X.shape)\n",
    "#print(y.shape)\n",
    "nsubset = 80000\n",
    "\n",
    "x_short = X[0:nsubset,:,:]\n",
    "y_short = y[0:nsubset,:]\n",
    "# x_short = X \n",
    "# y_short = y\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_short, y_short, test_size=0.5)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_vocab*4, input_shape=input_shape, return_sequences=True))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(LSTM(n_vocab*2))\n",
    "model.add(Dropout(0.15))\n",
    "#model.add(Dense(n_vocab*2))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.15))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=filename+\".{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "#callbacks_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12500 samples, validate on 12500 samples\n",
      "Epoch 1/100\n",
      "12500/12500 [==============================] - 23s 2ms/step - loss: 3.2324 - acc: 0.1588 - val_loss: 3.1504 - val_acc: 0.1627\n",
      "Epoch 2/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 3.1633 - acc: 0.1652 - val_loss: 3.1485 - val_acc: 0.1627\n",
      "Epoch 3/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 3.1511 - acc: 0.1653 - val_loss: 3.1452 - val_acc: 0.1627\n",
      "Epoch 4/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 3.1450 - acc: 0.1662 - val_loss: 3.1488 - val_acc: 0.1627\n",
      "Epoch 5/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 3.1352 - acc: 0.1662 - val_loss: 3.1385 - val_acc: 0.1627\n",
      "Epoch 6/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 3.1043 - acc: 0.1702 - val_loss: 3.0640 - val_acc: 0.1776\n",
      "Epoch 7/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 3.0346 - acc: 0.1974 - val_loss: 3.0153 - val_acc: 0.1970\n",
      "Epoch 8/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.9973 - acc: 0.2022 - val_loss: 2.9966 - val_acc: 0.2009\n",
      "Epoch 9/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.9757 - acc: 0.2051 - val_loss: 2.9756 - val_acc: 0.1944\n",
      "Epoch 10/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.9496 - acc: 0.2086 - val_loss: 2.9568 - val_acc: 0.2001\n",
      "Epoch 11/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.9234 - acc: 0.2068 - val_loss: 2.9265 - val_acc: 0.2094\n",
      "Epoch 12/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.8910 - acc: 0.2142 - val_loss: 2.8962 - val_acc: 0.2023\n",
      "Epoch 13/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.8518 - acc: 0.2177 - val_loss: 2.8617 - val_acc: 0.2118\n",
      "Epoch 14/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.8221 - acc: 0.2276 - val_loss: 2.8381 - val_acc: 0.2274\n",
      "Epoch 15/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.7952 - acc: 0.2260 - val_loss: 2.8144 - val_acc: 0.2287\n",
      "Epoch 16/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.7773 - acc: 0.2292 - val_loss: 2.8084 - val_acc: 0.2248\n",
      "Epoch 17/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.7616 - acc: 0.2362 - val_loss: 2.7916 - val_acc: 0.2301\n",
      "Epoch 18/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.7420 - acc: 0.2376 - val_loss: 2.7977 - val_acc: 0.2302\n",
      "Epoch 19/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.7238 - acc: 0.2411 - val_loss: 2.7826 - val_acc: 0.2339\n",
      "Epoch 20/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.7102 - acc: 0.2381 - val_loss: 2.7708 - val_acc: 0.2322\n",
      "Epoch 21/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.6973 - acc: 0.2462 - val_loss: 2.7676 - val_acc: 0.2321\n",
      "Epoch 22/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.6852 - acc: 0.2441 - val_loss: 2.7716 - val_acc: 0.2354\n",
      "Epoch 23/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.6635 - acc: 0.2515 - val_loss: 2.7670 - val_acc: 0.2341\n",
      "Epoch 24/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.6526 - acc: 0.2514 - val_loss: 2.7554 - val_acc: 0.2342\n",
      "Epoch 25/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.6301 - acc: 0.2590 - val_loss: 2.7545 - val_acc: 0.2395\n",
      "Epoch 26/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.6196 - acc: 0.2592 - val_loss: 2.7493 - val_acc: 0.2389\n",
      "Epoch 27/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.5904 - acc: 0.2634 - val_loss: 2.7476 - val_acc: 0.2358\n",
      "Epoch 28/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.5751 - acc: 0.2661 - val_loss: 2.7469 - val_acc: 0.2378\n",
      "Epoch 29/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.5517 - acc: 0.2734 - val_loss: 2.7616 - val_acc: 0.2368\n",
      "Epoch 30/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.5288 - acc: 0.2778 - val_loss: 2.7539 - val_acc: 0.2361\n",
      "Epoch 31/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.5015 - acc: 0.2826 - val_loss: 2.7592 - val_acc: 0.2326\n",
      "Epoch 32/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.4771 - acc: 0.2935 - val_loss: 2.7743 - val_acc: 0.2305\n",
      "Epoch 33/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.4460 - acc: 0.2952 - val_loss: 2.7778 - val_acc: 0.2339\n",
      "Epoch 34/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.4143 - acc: 0.3023 - val_loss: 2.7657 - val_acc: 0.2394\n",
      "Epoch 35/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.3805 - acc: 0.3117 - val_loss: 2.7784 - val_acc: 0.2348\n",
      "Epoch 36/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.3408 - acc: 0.3198 - val_loss: 2.7900 - val_acc: 0.2355\n",
      "Epoch 37/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.3087 - acc: 0.3285 - val_loss: 2.8129 - val_acc: 0.2290\n",
      "Epoch 38/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.2633 - acc: 0.3422 - val_loss: 2.8206 - val_acc: 0.2342\n",
      "Epoch 39/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.2206 - acc: 0.3542 - val_loss: 2.8260 - val_acc: 0.2333\n",
      "Epoch 40/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.1743 - acc: 0.3676 - val_loss: 2.8584 - val_acc: 0.2240\n",
      "Epoch 41/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.1291 - acc: 0.3780 - val_loss: 2.8656 - val_acc: 0.2235\n",
      "Epoch 42/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.0891 - acc: 0.3888 - val_loss: 2.9052 - val_acc: 0.2231\n",
      "Epoch 43/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 2.0358 - acc: 0.4065 - val_loss: 2.9138 - val_acc: 0.2188\n",
      "Epoch 44/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.9740 - acc: 0.4184 - val_loss: 2.9403 - val_acc: 0.2204\n",
      "Epoch 45/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.9310 - acc: 0.4310 - val_loss: 2.9955 - val_acc: 0.2213\n",
      "Epoch 46/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.8851 - acc: 0.4466 - val_loss: 2.9990 - val_acc: 0.2186\n",
      "Epoch 47/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.8396 - acc: 0.4546 - val_loss: 3.0364 - val_acc: 0.2160\n",
      "Epoch 48/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.7778 - acc: 0.4730 - val_loss: 3.0696 - val_acc: 0.2202\n",
      "Epoch 49/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.7385 - acc: 0.4838 - val_loss: 3.0873 - val_acc: 0.2124\n",
      "Epoch 50/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.6737 - acc: 0.5034 - val_loss: 3.1467 - val_acc: 0.2123\n",
      "Epoch 51/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.6246 - acc: 0.5175 - val_loss: 3.1766 - val_acc: 0.2072\n",
      "Epoch 52/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.5806 - acc: 0.5287 - val_loss: 3.2110 - val_acc: 0.2077\n",
      "Epoch 53/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.5125 - acc: 0.5502 - val_loss: 3.2305 - val_acc: 0.2087\n",
      "Epoch 54/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.4767 - acc: 0.5616 - val_loss: 3.3073 - val_acc: 0.2130\n",
      "Epoch 55/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.4235 - acc: 0.5770 - val_loss: 3.3406 - val_acc: 0.2120\n",
      "Epoch 56/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.3606 - acc: 0.5965 - val_loss: 3.3908 - val_acc: 0.2018\n",
      "Epoch 57/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.3274 - acc: 0.5990 - val_loss: 3.4396 - val_acc: 0.2006\n",
      "Epoch 58/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.2902 - acc: 0.6122 - val_loss: 3.4821 - val_acc: 0.1958\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.2249 - acc: 0.6342 - val_loss: 3.5250 - val_acc: 0.1981\n",
      "Epoch 60/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.1881 - acc: 0.6391 - val_loss: 3.5898 - val_acc: 0.1989\n",
      "Epoch 61/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.1581 - acc: 0.6518 - val_loss: 3.6291 - val_acc: 0.2030\n",
      "Epoch 62/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.1060 - acc: 0.6726 - val_loss: 3.6727 - val_acc: 0.2034\n",
      "Epoch 63/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.0764 - acc: 0.6745 - val_loss: 3.7251 - val_acc: 0.1988\n",
      "Epoch 64/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 1.0197 - acc: 0.6971 - val_loss: 3.7851 - val_acc: 0.1990\n",
      "Epoch 65/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.9991 - acc: 0.6992 - val_loss: 3.8244 - val_acc: 0.2014\n",
      "Epoch 66/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.9535 - acc: 0.7145 - val_loss: 3.9084 - val_acc: 0.1991\n",
      "Epoch 67/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.9176 - acc: 0.7253 - val_loss: 3.9117 - val_acc: 0.2002\n",
      "Epoch 68/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.8757 - acc: 0.7391 - val_loss: 4.0202 - val_acc: 0.1942\n",
      "Epoch 69/100\n",
      "12500/12500 [==============================] - 19s 2ms/step - loss: 0.8304 - acc: 0.7487 - val_loss: 4.0386 - val_acc: 0.1928\n",
      "Epoch 70/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.8120 - acc: 0.7555 - val_loss: 4.0886 - val_acc: 0.1987\n",
      "Epoch 71/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.7874 - acc: 0.7650 - val_loss: 4.1318 - val_acc: 0.1967\n",
      "Epoch 72/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.7538 - acc: 0.7754 - val_loss: 4.1607 - val_acc: 0.1929\n",
      "Epoch 73/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.7157 - acc: 0.7868 - val_loss: 4.2400 - val_acc: 0.2021\n",
      "Epoch 74/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.7137 - acc: 0.7862 - val_loss: 4.3047 - val_acc: 0.1922\n",
      "Epoch 75/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.6909 - acc: 0.7909 - val_loss: 4.3229 - val_acc: 0.1916\n",
      "Epoch 76/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.6614 - acc: 0.8002 - val_loss: 4.3671 - val_acc: 0.1931\n",
      "Epoch 77/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.6170 - acc: 0.8148 - val_loss: 4.4527 - val_acc: 0.1997\n",
      "Epoch 78/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.5989 - acc: 0.8204 - val_loss: 4.4954 - val_acc: 0.1896\n",
      "Epoch 79/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.6328 - acc: 0.8081 - val_loss: 4.5227 - val_acc: 0.1999\n",
      "Epoch 80/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.5505 - acc: 0.8394 - val_loss: 4.5951 - val_acc: 0.1906\n",
      "Epoch 81/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.5599 - acc: 0.8331 - val_loss: 4.6312 - val_acc: 0.1944\n",
      "Epoch 82/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.5362 - acc: 0.8446 - val_loss: 4.6883 - val_acc: 0.1933\n",
      "Epoch 83/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.5074 - acc: 0.8495 - val_loss: 4.7822 - val_acc: 0.1904\n",
      "Epoch 84/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.4821 - acc: 0.8594 - val_loss: 4.7690 - val_acc: 0.1943\n",
      "Epoch 85/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.4722 - acc: 0.8604 - val_loss: 4.8535 - val_acc: 0.1928\n",
      "Epoch 86/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.4902 - acc: 0.8544 - val_loss: 4.8512 - val_acc: 0.1896\n",
      "Epoch 87/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.4570 - acc: 0.8665 - val_loss: 4.8958 - val_acc: 0.1956\n",
      "Epoch 88/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.4372 - acc: 0.8714 - val_loss: 4.9411 - val_acc: 0.1941\n",
      "Epoch 89/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.4403 - acc: 0.8662 - val_loss: 4.9795 - val_acc: 0.1910\n",
      "Epoch 90/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.4591 - acc: 0.8618 - val_loss: 4.9813 - val_acc: 0.1942\n",
      "Epoch 91/100\n",
      "12500/12500 [==============================] - 20s 2ms/step - loss: 0.4422 - acc: 0.8671 - val_loss: 5.0221 - val_acc: 0.1851\n",
      "Epoch 92/100\n",
      " 6424/12500 [==============>...............] - ETA: 7s - loss: 0.3934 - acc: 0.8847"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size=173\n",
    "n_epochs=50\n",
    "\n",
    "h=model.fit(x_train, y_train, epochs=n_epochs, batch_size=batch_size, \n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_data(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Evaluating network...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-b38ce74e9450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate TEST model class prediction accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Evaluating network...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m print(classification_report(test_y.argmax(axis=1),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate TEST model class prediction accuracy\n",
    "print(\"[INFO] Evaluating network...\")\n",
    "predictions = model.predict(x_test, batch_size=batch_size)\n",
    "target_names = [str(x) for x in chars]\n",
    "print(classification_report(test_y.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1),\n",
    "                            target_names=cifar10_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_input(str, length):\n",
    "    return str.rjust(length)\n",
    "\n",
    "def str_to_int_sequence( a_string, char_to_int):\n",
    "    return [char_to_int[char] for char in a_string]\n",
    "\n",
    "def generate_text(model, num_chars, prompt, char_to_int):\n",
    "    \n",
    "    n_outputs = model.output_shape[1]\n",
    "    output_text = prompt\n",
    "    network_input = str_to_int_sequence(prompt.lower(), char_to_int)\n",
    "    \n",
    "    for i in range(num_chars):\n",
    "        x = np.reshape(network_input, (1, len(network_input), 1))\n",
    "        x = x / float(n_outputs)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_char[index]\n",
    "        output_text += result\n",
    "        seq_in = [int_to_char[value] for value in network_input]\n",
    "        network_input.append(index)\n",
    "        network_input = network_input[1:len(network_input)]\n",
    "        \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights for the final model\n",
    "#weights_file = \"ml.txt.10-2.8160.hdf5\"\n",
    "weights_file = \"ml.txt.10-0.1931.hdf5\"\n",
    "model.load_weights(weights_file)\n",
    "#model = load_model(weights_file)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "print(input_dim)\n",
    "#prompt = prompt_input('Machine learning is the use of computers', input_dim)\n",
    "#prompt = raw_text[0:input_dim]\n",
    "#prompt = prompt_input('Hey LSTM, tell us a bit about yourself. ', input_dim)\n",
    "prompt = prompt_input('Buddy, they wont even let me ', input_dim)\n",
    "print(prompt)\n",
    "print(len(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = generate_text(model=model, num_chars=144+input_shape[0], \n",
    "                     prompt=prompt, char_to_int=char_to_int)\n",
    "print(text)\n",
    "print(len(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
